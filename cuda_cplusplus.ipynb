{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kt-chan/cuda-demo/blob/main/cuda_cplusplus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hxH1vx9kA8g"
      },
      "source": [
        "# 使用Google Colab寫C++程式並運行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoFMIvx-kKi5"
      },
      "source": [
        "Create sample c++ code with %%writefile filename.cpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK0fQaW4kHu6"
      },
      "outputs": [],
      "source": [
        "%%writefile demo.cpp\n",
        "\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "int main()\n",
        "{\n",
        "    string text = \"world2\";\n",
        "    cout << \"hello, \" + text;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtYGbEoVkaFa"
      },
      "source": [
        "Compile the code with %%shell command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yl4tUf6nkdRm",
        "outputId": "fdc8fa6f-16f8-4b0c-cd21-db3ea9515626"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "g++ demo.cpp -o demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hziN6Cn6kla_"
      },
      "source": [
        "Execution by just run it, with %%shell command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6xM1r5Skkcl",
        "outputId": "7db344c6-b64f-4e31-8423-2c7e0f59c20f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World!\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "./demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoOTaMUckvVH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Svv4POytgLu"
      },
      "source": [
        "# 配置 CUDA Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeBPLY2jsBB8",
        "outputId": "34f7c8f1-c9f5-4b84-b4c3-ce787626c604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May 23 04:01:29 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check nvidia card info\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsym_6pxsmd1",
        "outputId": "ba7e0a99-9ec3-4be4-c446-f7212dfdd758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=22.04\n",
            "DISTRIB_CODENAME=jammy\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\n",
            "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ],
      "source": [
        "# check os info\n",
        "!cat /etc/*release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoNkF4uZwmLs",
        "outputId": "3f7d13a4-ca77-441e-ffb8-c3ab9015161f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "#get current working directory\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4HXqDEyt7Fi"
      },
      "source": [
        "Remote all legacy cuda framework, and update to latest version Go here: https://developer.nvidia.com/cuda-downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMZ4P3UQr0m7"
      },
      "outputs": [],
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2HgFrvX_tAfu"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
        "!sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/12.5.0/local_installers/cuda-repo-ubuntu2204-12-5-local_12.5.0-555.42.02-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu2204-12-5-local_12.5.0-555.42.02-1_amd64.deb\n",
        "!sudo cp /var/cuda-repo-ubuntu2204-12-5-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda-toolkit-12-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uILEWVo8uz41",
        "outputId": "fad7ea20-dac2-4bd4-d559-071631ecd170"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "#After refresh the cuda framework, check version info\n",
        "\n",
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkmrdk_tmuug"
      },
      "source": [
        "# Setup Google Colab for cuda c++, rerun this for new session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prUP20qzm2S0"
      },
      "source": [
        "set your runtime to cuda by click \"runtime\" -> \"change runtime type\" in above toolbar, and select T4 GPU.\n",
        "\n",
        "First, you have to install nvcc plugin for cuda compiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AruXtVyMnEH1",
        "outputId": "16d687bf-1c54-4909-dba6-890ec6cd47d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRhcZ8t-n-zI"
      },
      "source": [
        "then, Load the plugin\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2rmueJ_oBV1",
        "outputId": "4b737bb8-f0da-46f3-e760-5d36f5e15e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpfhvxfg1x\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69a-ObGBnIxJ",
        "outputId": "af9d80ec-a407-4e3e-973a-31a9a3bac46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "# check nvidia card info\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h5KCnsbnR1I"
      },
      "source": [
        "# Demo Code - Simple Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SimpleLogger.h\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <ctime>\n",
        "#include <iomanip>\n",
        "#include <sstream>\n",
        "#include <stdexcept>\n",
        "#include <cstdarg>  // Correct header for va_list and related macros\n",
        "#include <cstdio>\n",
        "\n",
        "enum LogLevel {\n",
        "\tDEBUG,\n",
        "\tINFO,\n",
        "\tWARN,\n",
        "\tERROR,\n",
        "\tFATAL\n",
        "};\n",
        "\n",
        "class SimpleLogger {\n",
        "private:\n",
        "\tLogLevel currentLevel;\n",
        "\tstd::string logFormat; // Stores the format for log messages\n",
        "\n",
        "\t// Helper function to convert LogLevel to string\n",
        "\tstd::string LogLevelToString(LogLevel level) {\n",
        "\t\tswitch (level) {\n",
        "\t\tcase DEBUG: return \"DEBUG\";\n",
        "\t\tcase INFO: return \"INFO\";\n",
        "\t\tcase WARN: return \"WARN\";\n",
        "\t\tcase ERROR: return \"ERROR\";\n",
        "\t\tcase FATAL: return \"FATAL\";\n",
        "\t\tdefault: return \"UNKNOWN\";\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "public:\n",
        "\t// Constructor that accepts an initial log level and an optional log format\n",
        "\tSimpleLogger(LogLevel level, const std::string& format = \"[%Y-%m-%d %H:%M:%S] [%L] %M\")\n",
        "\t\t: currentLevel(level), logFormat(format) {}\n",
        "\n",
        "\tvoid SetLogLevel(LogLevel level) {\n",
        "\t\tcurrentLevel = level;\n",
        "\t}\n",
        "\n",
        "\t// Function to set the log format\n",
        "\tvoid SetLogFormat(const std::string& format) {\n",
        "\t\tlogFormat = format;\n",
        "\t}\n",
        "\n",
        "\t// Printf-style log function\n",
        "\tvoid Logf(LogLevel level, const char* format, ...) {\n",
        "\t\tif (level < currentLevel) return;\n",
        "\n",
        "\t\tchar buffer[1024];\n",
        "\t\tstd::va_list args;\n",
        "\t\tva_start(args, format);\n",
        "\t\t// Use std::vsnprintf to safely format the string into the buffer\n",
        "\t\tstd::vsnprintf(buffer, sizeof(buffer), format, args);\n",
        "\t\tva_end(args);\n",
        "\n",
        "\t\t// Now log the formatted message using the existing Log method\n",
        "\t\tLog(level, buffer);\n",
        "\t}\n",
        "\n",
        "\t// Function to log messages with the given log level\n",
        "\tvoid Log(LogLevel level, const int& message) {\n",
        "\t\tLog(level, std::to_string(message));\n",
        "\t}\n",
        "\n",
        "\t// Function to log messages with the given log level\n",
        "\tvoid Log(LogLevel level, const std::string& message) {\n",
        "\t\tif (level < currentLevel) return; // Skip messages below the current log level\n",
        "\n",
        "\t\t// Get the current time\n",
        "\t\tstd::time_t now = std::time(nullptr);\n",
        "\t\tstd::tm timeInfo;\n",
        "\t\t// Use localtime_s on MSVC or localtime on other platforms\n",
        "#ifdef _MSC_VER\n",
        "\t\tlocaltime_s(&timeInfo, &now);\n",
        "#else\n",
        "\t\tstd::tm* timeInfoPtr = std::localtime(&now);\n",
        "\t\tif (!timeInfoPtr) return; // If localtime fails, return\n",
        "\t\ttimeInfo = *timeInfoPtr;\n",
        "#endif\n",
        "\n",
        "\t\t// Replace format specifiers with actual values\n",
        "\t\tstd::string formattedMessage = logFormat;\n",
        "\t\tsize_t pos = 0;\n",
        "\t\tif ((pos = formattedMessage.find(\"%Y\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 3, std::to_string(timeInfo.tm_year + 1900));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%m\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_mon + 1));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%d\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_mday));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%H\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_hour));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%M\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_min));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%S\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_sec));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%L\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, LogLevelToString(level));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%M\")) != std::string::npos) {\n",
        "\t\t\tsize_t end = pos + 2;\n",
        "\t\t\tformattedMessage.replace(pos, end - pos, message);\n",
        "\t\t}\n",
        "\n",
        "\t\t// Output the formatted log message to the console with endl;\n",
        "\t\tstd::cout << formattedMessage << std::endl;\n",
        "\n",
        "\t}\n",
        "};\n",
        "\n",
        "// Declare a static instance of SimpleLogger\n",
        "static SimpleLogger& GlobalLogger(LogLevel  level = INFO) {\n",
        "\tstatic SimpleLogger logger(level);\n",
        "\treturn logger;\n",
        "}\n"
      ],
      "metadata": {
        "id": "q3y3Z0OOPYXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG3x_MGkxy0i"
      },
      "source": [
        "Simple 1 layer loop demo\n",
        "\n",
        "// %%cuda_group_save -n demo.cu -g share"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Kzce0fYeKb2i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0111ca6b-b895-48f0-fcef-0b0a10c8f9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demo.cpp\n"
          ]
        }
      ],
      "source": [
        "%%writefile demo.cpp\n",
        "\n",
        "#define _CRT_SECURE_NO_WARNINGS\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <stdexcept>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include \"SimpleLogger.h\"\n",
        "\n",
        "\n",
        "#define Debug false\n",
        "#define K 8\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "class demo {\n",
        "\n",
        "\n",
        "public:\n",
        "\n",
        "\tvoid genRandomNumber(int* a, int n) {\n",
        "\t\tfor (int i = 0; i < n; i++) {\n",
        "\t\t\ta[i] = rand() % n;\n",
        "\t\t\tGlobalLogger().Log(DEBUG, a[i]);\n",
        "\t\t}\n",
        "\t\tGlobalLogger().Log(DEBUG, \"\\n\");\n",
        "\t\tGlobalLogger().Log(INFO, \"first elment: \" + to_string(a[1]) + \"...\");\n",
        "\t}\n",
        "\n",
        "\n",
        "\tvoid cpucal(int* a, int* b, long long& c, int n)\n",
        "\t{\n",
        "\t\tlong long  sum = 0;\n",
        "\t\t// calculate the dot product of two array\n",
        "\t\tfor (int i = 0; i < n; i++)\n",
        "\t\t{\n",
        "\t\t\tsum += (a[i] * b[i]);\n",
        "\t\t}\n",
        "\t\tc = sum;\n",
        "\t}\n",
        "\n",
        "\n",
        "\n",
        "\tbool checkGPU()\n",
        "\t{\n",
        "\t\tconst char* gpu_env = std::getenv(\"COLAB_GPU\");\n",
        "\t\tif (gpu_env && atoi(gpu_env) > 0)\n",
        "\t\t{\n",
        "\t\t\tGlobalLogger().Log(INFO, \"A GPU is connected.\");\n",
        "\t\t\treturn true;\n",
        "\t\t}\n",
        "\t\telse\n",
        "\t\t{\n",
        "\t\t\tGlobalLogger().Log(INFO, \"No accelerator is connected.\");\n",
        "\t\t\treturn false;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tint run(void) {\n",
        "\n",
        "\t\tcheckGPU();\n",
        "\n",
        "\t\tunsigned int N = 1 << K;\n",
        "\t\tGlobalLogger().Logf(INFO, \"Total array size: %d\\n\", N);\n",
        "\n",
        "\t\tint* a, * b;\n",
        "\t\tlong long cpu_output;\n",
        "\n",
        "\t\ta = (int*)malloc(N * sizeof(int));\n",
        "\t\tb = (int*)malloc(N * sizeof(int));\n",
        "\n",
        "\t\tGlobalLogger().Log(INFO, \"value of array a:\\t\");\n",
        "\t\tgenRandomNumber(a, N);\n",
        "\t\tGlobalLogger().Log(INFO, \"value of array b:\\t\");\n",
        "\t\tgenRandomNumber(b, N);\n",
        "\n",
        "\t\tGlobalLogger().Log(INFO, \"@CPU, summing value...\");\n",
        "\n",
        "\t\tclock_t t;\n",
        "\n",
        "\t\t// calling cpu\n",
        "\t\tt = clock();//start time\n",
        "\t\tcpucal(a, b, cpu_output, N);\n",
        "\t\tt = clock() - t;//total time = end time - start time\n",
        "\t\tGlobalLogger().Log(INFO, \"result:  \" + to_string(cpu_output));\n",
        "\t\tGlobalLogger().Logf(INFO, \"CPU Avg time = %lf ms.\\n\", ((((float)t) / CLOCKS_PER_SEC) * 1000));\n",
        "\n",
        "\t\tcudaFree(a);\n",
        "\t\tcudaFree(b);\n",
        "\n",
        "\t\treturn 0;\n",
        "\t}\n",
        "};\n",
        "\n",
        "\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tGlobalLogger(INFO);\n",
        "\tGlobalLogger().Log(INFO, \"Application started. This is info level log\");\n",
        "\tGlobalLogger().Log(DEBUG, \"This is a debug message.\"); // This will not be shown because it's below the INFO level\n",
        "\n",
        "\tdemo demoapp;\n",
        "\tdemoapp.run();\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D-M3lvLK4hD",
        "outputId": "908ac187-0435-4714-b03a-e5e87edbf30d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20245-28 8:6:59] [INFO] Application started. This is info level log\n",
            "[20245-28 8:6:59] [INFO] A GPU is connected.\n",
            "[20245-28 8:6:59] [INFO] Total array size: 256\n",
            "\n",
            "[20245-28 8:6:59] [INFO] value of array a:\t\n",
            "[20245-28 8:6:59] [INFO] first elment: 198...\n",
            "[20245-28 8:6:59] [INFO] value of array b:\t\n",
            "[20245-28 8:6:59] [INFO] first elment: 112...\n",
            "[20245-28 8:6:59] [INFO] @CPU, summing value...\n",
            "[20245-28 8:6:59] [INFO] result:  4214798\n",
            "[20245-28 8:6:59] [INFO] CPU Avg time = 0.002000 ms.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc -o demo demo.cpp\n",
        "./demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3RfhhKTltBT"
      },
      "source": [
        "# Demo Code - Complex nested loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile SimpleLogger.h\n",
        "#include <iostream>\n",
        "#include <string>\n",
        "#include <ctime>\n",
        "#include <iomanip>\n",
        "#include <sstream>\n",
        "#include <stdexcept>\n",
        "#include <cstdarg>  // Correct header for va_list and related macros\n",
        "#include <cstdio>\n",
        "\n",
        "enum LogLevel {\n",
        "\tDEBUG,\n",
        "\tINFO,\n",
        "\tWARN,\n",
        "\tERROR,\n",
        "\tFATAL\n",
        "};\n",
        "\n",
        "class SimpleLogger {\n",
        "private:\n",
        "\tLogLevel currentLevel;\n",
        "\tstd::string logFormat; // Stores the format for log messages\n",
        "\n",
        "\t// Helper function to convert LogLevel to string\n",
        "\tstd::string LogLevelToString(LogLevel level) {\n",
        "\t\tswitch (level) {\n",
        "\t\tcase DEBUG: return \"DEBUG\";\n",
        "\t\tcase INFO: return \"INFO\";\n",
        "\t\tcase WARN: return \"WARN\";\n",
        "\t\tcase ERROR: return \"ERROR\";\n",
        "\t\tcase FATAL: return \"FATAL\";\n",
        "\t\tdefault: return \"UNKNOWN\";\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "public:\n",
        "\t// Constructor that accepts an initial log level and an optional log format\n",
        "\tSimpleLogger(LogLevel level, const std::string& format = \"[%Y-%m-%d %H:%M:%S] [%L] %M\")\n",
        "\t\t: currentLevel(level), logFormat(format) {}\n",
        "\n",
        "\tvoid SetLogLevel(LogLevel level) {\n",
        "\t\tcurrentLevel = level;\n",
        "\t}\n",
        "\n",
        "\t// Function to set the log format\n",
        "\tvoid SetLogFormat(const std::string& format) {\n",
        "\t\tlogFormat = format;\n",
        "\t}\n",
        "\n",
        "\t// Printf-style log function\n",
        "\tvoid Logf(LogLevel level, const char* format, ...) {\n",
        "\t\tif (level < currentLevel) return;\n",
        "\n",
        "\t\tchar buffer[1024];\n",
        "\t\tstd::va_list args;\n",
        "\t\tva_start(args, format);\n",
        "\t\t// Use std::vsnprintf to safely format the string into the buffer\n",
        "\t\tstd::vsnprintf(buffer, sizeof(buffer), format, args);\n",
        "\t\tva_end(args);\n",
        "\n",
        "\t\t// Now log the formatted message using the existing Log method\n",
        "\t\tLog(level, buffer);\n",
        "\t}\n",
        "\n",
        "\t// Function to log messages with the given log level\n",
        "\tvoid Log(LogLevel level, const int& message) {\n",
        "\t\tLog(level, std::to_string(message));\n",
        "\t}\n",
        "\n",
        "\t// Function to log messages with the given log level\n",
        "\tvoid Log(LogLevel level, const std::string& message) {\n",
        "\t\tif (level < currentLevel) return; // Skip messages below the current log level\n",
        "\n",
        "\t\t// Get the current time\n",
        "\t\tstd::time_t now = std::time(nullptr);\n",
        "\t\tstd::tm timeInfo;\n",
        "\t\t// Use localtime_s on MSVC or localtime on other platforms\n",
        "#ifdef _MSC_VER\n",
        "\t\tlocaltime_s(&timeInfo, &now);\n",
        "#else\n",
        "\t\tstd::tm* timeInfoPtr = std::localtime(&now);\n",
        "\t\tif (!timeInfoPtr) return; // If localtime fails, return\n",
        "\t\ttimeInfo = *timeInfoPtr;\n",
        "#endif\n",
        "\n",
        "\t\t// Replace format specifiers with actual values\n",
        "\t\tstd::string formattedMessage = logFormat;\n",
        "\t\tsize_t pos = 0;\n",
        "\t\tif ((pos = formattedMessage.find(\"%Y\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 3, std::to_string(timeInfo.tm_year + 1900));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%m\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_mon + 1));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%d\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_mday));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%H\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_hour));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%M\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_min));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%S\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, std::to_string(timeInfo.tm_sec));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%L\")) != std::string::npos) {\n",
        "\t\t\tformattedMessage.replace(pos, 2, LogLevelToString(level));\n",
        "\t\t}\n",
        "\t\tif ((pos = formattedMessage.find(\"%M\")) != std::string::npos) {\n",
        "\t\t\tsize_t end = pos + 2;\n",
        "\t\t\tformattedMessage.replace(pos, end - pos, message);\n",
        "\t\t}\n",
        "\n",
        "\t\t// Output the formatted log message to the console with endl;\n",
        "\t\tstd::cout << formattedMessage << std::endl;\n",
        "\n",
        "\t}\n",
        "};\n",
        "\n",
        "// Declare a static instance of SimpleLogger\n",
        "static SimpleLogger& GlobalLogger(LogLevel  level = INFO) {\n",
        "\tstatic SimpleLogger logger(level);\n",
        "\treturn logger;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3QHkTQbPh-s",
        "outputId": "357115dc-10d5-4dad-eb74-229052a1d1f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing SimpleLogger.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssjCk0I8x2aO"
      },
      "source": [
        "Complex N layer loop demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0UeqHAppx59o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8829d7b5-1865-4a16-feb0-25013447f584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing demo.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile demo.cu\n",
        "\n",
        "#define _CRT_SECURE_NO_WARNINGS\n",
        "#include <cstdlib>\n",
        "#include <iostream>\n",
        "#include <stdexcept>\n",
        "#include <cuda_runtime.h>\n",
        "#include <cuda_runtime_api.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <random>\n",
        "#include \"SimpleLogger.h\"\n",
        "\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__ void gpucal_partial_kernel(int* a, int* b, int* c, int n)\n",
        "{\n",
        "\tint threadId = threadIdx.x + blockDim.x * blockIdx.x;\n",
        "\tc[threadId] += (a[threadId] * b[threadId]);\n",
        "}\n",
        "\n",
        "\n",
        "class DemoCuda {\n",
        "private:\n",
        "\tvoid checkCudaError(cudaError_t error) {\n",
        "\t\tif (error != cudaSuccess) {\n",
        "\t\t\tGlobalLogger().Logf(ERROR, \"CUDA Error: %s\", cudaGetErrorString(error));\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tvoid checkCudaError(cudaError_t error, string stmt) {\n",
        "\t\tif (error != cudaSuccess) {\n",
        "\t\t\tGlobalLogger().Logf(ERROR, \"CUDA Statement: %s\", stmt);\n",
        "\t\t\tcheckCudaError(error);\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tvoid genRandomNumber(int* a, int n) {\n",
        "\t\trandom_device rd; // Create a random device and use it to generate a seed\n",
        "\t\tmt19937 gen(rd()); // Mersenne Twister generator with a random seed\n",
        "\t\tuniform_int_distribution<> distr(-128, 127); // Generate a random number， with in  the range\n",
        "\n",
        "\t\tfor (int i = 0; i < n; i++) {\n",
        "\t\t\tint random_number = distr(gen); // Generate the number\n",
        "\t\t\ta[i] = random_number;\n",
        "\t\t\tGlobalLogger().Log(DEBUG, a[i]);\n",
        "\t\t}\n",
        "\t\tGlobalLogger().Log(DEBUG, \"\\n\");\n",
        "\t\tGlobalLogger().Log(INFO, \"first elment: \" + to_string(a[0]) + \"...\");\n",
        "\t}\n",
        "\n",
        "\tvoid cpucal(int* a, int* b, int& c, int n)\n",
        "\t{\n",
        "\t\tint  sum = 0;\n",
        "\t\t// calculate the dot product of two array\n",
        "\t\tfor (int i = 0; i < n; i++)\n",
        "\t\t{\n",
        "\t\t\tsum += (a[i] * b[i]);\n",
        "\t\t}\n",
        "\t\tc = sum;\n",
        "\t}\n",
        "\n",
        "\n",
        "\n",
        "\tvoid gpucal(int* a, int* b, long long& c, int n) {\n",
        "\t\tint* d_a, * d_b;\n",
        "\t\tint* d_c;\n",
        "\n",
        "\t\t// Attempt to allocate memory on the host\n",
        "\t\tcheckCudaError(cudaMalloc(&d_a, n * sizeof(int)), \"cudaMalloc(&d_a, N * sizeof(int))\");\n",
        "\t\tcudaMemcpy(d_a, a, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\t\tcheckCudaError(cudaFreeHost(a), \"cudaFreeHost(a)\");\n",
        "\n",
        "\t\tcheckCudaError(cudaMalloc(&d_b, n * sizeof(int)), \"cudaMalloc(&d_b, N * sizeof(int))\");\n",
        "\t\tcudaMemcpy(d_b, b, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\t\tcheckCudaError(cudaFreeHost(b), \"cudaFreeHost(b)\");\n",
        "\n",
        "\t\tcheckCudaError(cudaMalloc(&d_c, n * sizeof(int)), \"cudaMalloc(&d_c, n * sizeof(int))\");\n",
        "\t\tcudaMemset(d_c, 0, n * sizeof(int));\n",
        "\n",
        "\t\tint* ctmp;\n",
        "\t\tcheckCudaError(cudaMallocHost(&ctmp, n * sizeof(int)), \"cudaMallocHost(&ctmp, n * sizeof(int))\");\n",
        "\n",
        "\n",
        "\t\t// define kernel call\n",
        "\t\tint grids = max(1, (n + 255) / 256);\n",
        "\t\tint blocks = max(1, min(n, 256));\n",
        "\n",
        "\t\tGlobalLogger().Logf(INFO, \"start to run gpucal_partial_kernel with <<<%d, %d>>>:  \", grids, blocks);\n",
        "\t\t//gpucal_partial_kernel <<<grids, blocks>>> (d_a, d_b, d_c, n);\n",
        "\t\tgpucal_partial_kernel <<<grids, blocks>>> (d_a, d_b, d_c, n);\n",
        "\n",
        "\t\t// Sync Device to Host\n",
        "\t\tGlobalLogger().Logf(INFO, \"start to run cudaDeviceSynchronize ... \");\n",
        "\t\tcudaDeviceSynchronize();\n",
        "\n",
        "\t\tcudaMemcpy(ctmp, d_c, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\t\tGlobalLogger().Logf(INFO, \"start to run cudaMemcpy with c array first element: %lld\", ctmp[0]);\n",
        "\n",
        "\t\t// Sum up the partial sums on the host to get the final result\n",
        "\t\tGlobalLogger().Logf(DEBUG, \"Sum up the partial sums  ....\");\n",
        "\t\tlong long sum = 0;\n",
        "\t\tfor (int i = 0; i < n; ++i) {\n",
        "\t\t\tGlobalLogger().Logf(DEBUG, \"c[i]: %d\", ctmp[i]);\n",
        "\t\t\tsum += ctmp[i];\n",
        "\t\t\tGlobalLogger().Logf(DEBUG, \"sum: %lld\", sum);\n",
        "\t\t}\n",
        "\n",
        "\t\tGlobalLogger().Logf(INFO, \"Sum value: %lld\", sum);\n",
        "\n",
        "\t\t// Write the final sum to c[0]\n",
        "\t\tc = sum;\n",
        "\t\tGlobalLogger().Logf(INFO, \"gpucal return value: %lld\", c);\n",
        "\n",
        "\n",
        "\t\tcheckCudaError(cudaFree(d_a), \"cudaFree(d_a)\");\n",
        "\t\tcheckCudaError(cudaFree(d_b), \"cudaFree(d_b)\");\n",
        "\t\tcheckCudaError(cudaFree(d_c), \"cudaFree(d_c)\");\n",
        "\t}\n",
        "\n",
        "\tbool checkGPU()\n",
        "\t{\n",
        "\t\tconst char* gpu_env = getenv(\"COLAB_GPU\");\n",
        "\t\tif (gpu_env && atoi(gpu_env) > 0)\n",
        "\t\t{\n",
        "\t\t\tGlobalLogger().Log(INFO, \"A GPU is connected.\");\n",
        "\t\t\treturn true;\n",
        "\t\t}\n",
        "\t\telse\n",
        "\t\t{\n",
        "\t\t\tGlobalLogger().Log(INFO, \"No accelerator is connected.\");\n",
        "\t\t\treturn false;\n",
        "\t\t}\n",
        "\t}\n",
        "public:\n",
        "\n",
        "\tint run(int K = 8)\n",
        "\t{\n",
        "\n",
        "\t\tbool GPU = checkGPU();\n",
        "\n",
        "\t\tunsigned int N = 1 << K;\n",
        "\t\tGlobalLogger().Logf(INFO, \"Total array size: %d\\n\", N);\n",
        "\n",
        "\t\tint* a, * b;\n",
        "\t\tint cpu_output;\n",
        "\n",
        "\t\tif (!GPU)\n",
        "\t\t{\n",
        "\t\t\tGlobalLogger().Log(INFO, \"allocating cpu memory ... \");\n",
        "\t\t\ta = (int*)malloc(N * sizeof(int));\n",
        "\t\t\tb = (int*)malloc(N * sizeof(int));\n",
        "\t\t}\n",
        "\t\telse\n",
        "\t\t{\n",
        "\t\t\tGlobalLogger().Log(INFO, \"allocating gpu memory ... \");\n",
        "\t\t\t// Attempt to allocate memory on the host\n",
        "\t\t\tcheckCudaError(cudaMallocHost(&a, N * sizeof(int)));\n",
        "\t\t\tcheckCudaError(cudaMallocHost(&b, N * sizeof(int)));\n",
        "\t\t}\n",
        "\n",
        "\t\tGlobalLogger().Log(INFO, \"value of array a:\\t\");\n",
        "\t\tgenRandomNumber(a, N);\n",
        "\t\tGlobalLogger().Log(INFO, \"value of array b:\\t\");\n",
        "\t\tgenRandomNumber(b, N);\n",
        "\n",
        "\t\tGlobalLogger().Log(INFO, \"@CPU, finding dot product for value of size(n) * size(n) ... \\n\");\n",
        "\n",
        "\t\tclock_t t;\n",
        "\n",
        "\t\t// calling cpu\n",
        "\t\tt = clock(); // start time\n",
        "\t\tcpucal(a, b, cpu_output, N);\n",
        "\t\tt = clock() - t; // total time = end time - start time\n",
        "\n",
        "\t\tGlobalLogger().Log(INFO, \"result: \" + to_string(cpu_output));\n",
        "\n",
        "\t\tGlobalLogger().Logf(INFO, \"@CPU Avg time = %lf ms.\\n\", ((((float)t) / CLOCKS_PER_SEC) * 1000));\n",
        "\t\tif (GPU) {\n",
        "\t\t\tGlobalLogger().Log(INFO, \"@GPU, finding dot product for value of size(n) * size(n) ... \");\n",
        "\n",
        "\t\t\t// reset value c[0]\n",
        "\t\t\tlong long gpu_output;\n",
        "\t\t\t/*checkCudaError(cudaMallocHost(&gpu_output, 1 * sizeof(int)));\n",
        "\t\t\tmemset(gpu_output, 0, 1 * sizeof(int));*/\n",
        "\n",
        "\t\t\t// calling cpu\n",
        "\t\t\tt = clock(); // start time\n",
        "\t\t\tgpucal(a, b, gpu_output, N);\n",
        "\t\t\tt = clock() - t; // total time = end time - start time\n",
        "\n",
        "\t\t\tGlobalLogger().Log(INFO, \"result: \" + to_string(gpu_output));\n",
        "\t\t\tGlobalLogger().Logf(INFO, \"@CPU Avg time = %lf ms.\\n\", ((((float)t) / CLOCKS_PER_SEC) * 1000));\n",
        "\t\t\t/*cudaFreeHost(gpu_output);*/\n",
        "\n",
        "\t\t}\n",
        "\n",
        "\t\tif (!GPU)\n",
        "\t\t{\n",
        "\t\t\tfree(a);\n",
        "\t\t\tfree(b);\n",
        "\t\t}\n",
        "\t\telse\n",
        "\t\t{\n",
        "\t\t\tcudaFreeHost(a);\n",
        "\t\t\tcudaFreeHost(b);\n",
        "\t\t}\n",
        "\n",
        "\t\treturn 0;\n",
        "\t}\n",
        "};\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "\n",
        "\tGlobalLogger(INFO);\n",
        "\tGlobalLogger().Log(INFO, \"Application started. Logging at LogLevel: INFO\");\n",
        "\tfor (int i = 0; i < argc; ++i) {\n",
        "\t\tGlobalLogger().Logf(INFO, \"Argument %d: %s\", i, argv[i]);\n",
        "\t}\n",
        "\n",
        "\tDemoCuda app;\n",
        "\tapp.run(std::stoi(argv[1]));\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zx0KJFexx80D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fab73d9-66cc-4b1c-bf61-b4f1cd3cbd23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%%shell\n",
        "nvcc -o demo demo.cu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "./demo 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuawVY99iGM1",
        "outputId": "196f85b4-99cb-4cfd-fafe-e07e8af8575b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[20245-28 9:47:6] [INFO] Application started. Logging at LogLevel: INFO\n",
            "[20245-28 9:47:6] [INFO] Argument 0: ./demo\n",
            "[20245-28 9:47:6] [INFO] Argument 1: 30\n",
            "[20245-28 9:47:6] [INFO] A GPU is connected.\n",
            "[20245-28 9:47:6] [INFO] Total array size: 1073741824\n",
            "\n",
            "[20245-28 9:47:6] [INFO] allocating gpu memory ... \n",
            "[20245-28 9:47:11] [INFO] value of array a:\t\n",
            "[20245-28 9:49:1] [INFO] first elment: 126...\n",
            "[20245-28 9:49:1] [INFO] value of array b:\t\n",
            "[20245-28 9:50:51] [INFO] first elment: 88...\n",
            "[20245-28 9:50:51] [INFO] @CPU, finding dot product for value of size(n) * size(n) ... \n",
            "\n",
            "[20245-28 9:50:54] [INFO] result: 98790523\n",
            "[20245-28 9:50:54] [INFO] @CPU Avg time = 2867.577148 ms.\n",
            "\n",
            "[20245-28 9:50:54] [INFO] @GPU, finding dot product for value of size(n) * size(n) ... \n",
            "[20245-28 9:50:58] [INFO] start to run gpucal_partial_kernel with <<<4194304, 256>>>:  \n",
            "[20245-28 9:50:58] [INFO] start to run cudaDeviceSynchronize ... \n",
            "[20245-28 9:50:59] [INFO] start to run cudaMemcpy with c array first element: 11088\n",
            "[20245-28 9:51:15] [INFO] Sum value: 98790523\n",
            "[20245-28 9:51:15] [INFO] gpucal return value: 98790523\n",
            "[20245-28 9:51:15] [INFO] result: 98790523\n",
            "[20245-28 9:51:15] [INFO] @CPU Avg time = 21336.050781 ms.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "4h5KCnsbnR1I"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}